{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Link to the streamlit App**\n",
        "\n",
        "https://mlassignment3-error404.streamlit.app/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56g5g4O0hHtM",
        "outputId": "2ee93097-8cd5-45a5-cd8b-b2827a33a411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarrow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyarrow) (1.26.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: huggingface-hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.26.1)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub) (2024.8.30)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyarrow\n",
        "%pip install huggingface-hub\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "eb5ba447-2798-4157-af48-3f069d43a3ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.lm import Vocabulary\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# import nltk\n",
        "# nltk.download('punkt_tab')\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpcCqDMWhvSL",
        "outputId": "cbb936d0-0252-4a41-e913-43d5fd257344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_xla in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch_xla) (2.1.0)\n",
            "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch_xla) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch_xla) (6.0.2)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch_xla) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch_xla) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch_xla) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch_xla) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torch_xla) (2024.8.30)\n",
            "CUDA is available. Using GPU.\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  !pip install torch_xla\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  device = xm.xla_device()\n",
        "  print(\"TPU is available. Using TPU\")\n",
        "except:\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA not available. Using CPU.\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will train eight models, with two block sizes, two embedding dimensions, and two activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9v9qhbFahSTL"
      },
      "outputs": [],
      "source": [
        "block_sizes = [16, 32]\n",
        "emb_dims = [128, 256]\n",
        "layer_sizes = [512, 512, 512]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our dataset consists of Middle school level maths questions, which is provided online free of cost by microsoft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FBY_ktxRiI86"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"hf://datasets/microsoft/orca-math-word-problems-200k/data/train-00000-of-00001.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will tokenize these symbols separately, i.e. if \"10%2\" is present in the text, it will be converted to \"1 0 % 2\". The word_tokenize function basically splits using the blankspace and removes the empty tokens and newline characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Id8n4Y-5iPBg"
      },
      "outputs": [],
      "source": [
        "symbols = r\"([+\\-/*^=0123456789.,|'()])\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lWJzsPBUiRKh"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    text = text.replace(\"\\n\", \" NEWLINE_CHAR \")\n",
        "    text = re.sub(symbols, r\" \\1 \", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [\"\\n\" if token == \"NEWLINE_CHAR\" else token for token in tokens]\n",
        "    tokens.append('\\n')\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We make a list of lists of questions tokenized. We also save the unique tokens in the tokenset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rbzIsxaiTHs",
        "outputId": "ceeec7e6-9bc5-4fe6-edf0-00fc1807d5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n",
            "180000\n",
            "190000\n",
            "200000\n"
          ]
        }
      ],
      "source": [
        "str_arr = []\n",
        "tokenset=set()\n",
        "for i in range(len(df)):\n",
        "    token_arr = tokenize(df.iloc[i, 0])\n",
        "    str_arr.append(token_arr)\n",
        "    tokenset.update(token_arr)\n",
        "    if i%10000 == 0:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add an extra token \"END\" to signify the filler token, and make a dictionary mapping from tokens to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OPQtjh4iVqZ",
        "outputId": "e25c97ff-6148-4468-8cf7-302fffb3325d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26605\n"
          ]
        }
      ],
      "source": [
        "all_words = ['END']+sorted(list(tokenset))\n",
        "stoi = {s:i for i, s in enumerate(all_words)}\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "int_text_arr = [[stoi[i] for i in str_arr[j]] for j in range(len(str_arr))]\n",
        "print(len(all_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a NextWord model class, using which we make pytorch model objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HTLwCikailjm"
      },
      "outputs": [],
      "source": [
        "class NextWord(nn.Module):\n",
        "    def __init__(self, block_size, vocab_size, emb_dim, hidden_size, activation):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lin_1 = nn.Linear(block_size * emb_dim, hidden_size[0])\n",
        "        self.lin_2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
        "        self.lin_3 = nn.Linear(hidden_size[1], hidden_size[2])\n",
        "        # self.lin_1 = nn.Linear(block_size * emb_dim, hidden_size)\n",
        "        self.lin_4 = nn.Linear(hidden_size[2], vocab_size)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.activation(self.lin_1(x))\n",
        "        x = self.activation(self.lin_2(x))\n",
        "        x = self.activation(self.lin_3(x))\n",
        "        x = self.lin_4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below makes (and trains) a model with a specified context (block) size, embedding dimension, hidden layer sizes, and the activation. We will be using three hidden layers of 512, 512, 512 nodes each for every model of ours. We will vary the embedding dimensions, the context window, and the activation function applied to every node. We have a constant learning rate of 0.001, use the Adam optimizer and train it for 20 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our training dataset consists of 1,20,00,000 samples. (Which is the total number of words in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XdaxBETzhrqC"
      },
      "outputs": [],
      "source": [
        "def makemodel(model_name, block_size, emb_dim, hidden_size, activation, path=\"\", lr=0.001, epochs = 20):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range(len(int_text_arr)):\n",
        "      context =[0]*block_size\n",
        "      for j in range(len(int_text_arr[i])):\n",
        "          Y.append(int_text_arr[i][j])\n",
        "          X.append(context)\n",
        "          context = context[1:] + [int_text_arr[i][j]]\n",
        "      X.append(context)\n",
        "      Y.append(0)\n",
        "      if i%10000 == 0:\n",
        "          print(i)\n",
        "  X = torch.tensor(X).to(device)\n",
        "  Y = torch.tensor(Y).to(device)\n",
        "  torch.manual_seed(42)\n",
        "  perm = torch.randperm(X.shape[0])\n",
        "  X = X[perm]\n",
        "  Y = Y[perm]\n",
        "  print(len(X))\n",
        "  def load_model(path = \"\"):\n",
        "    if path == \"\":\n",
        "        model = NextWord(block_size, len(stoi), emb_dim, hidden_size, activation).to(device)\n",
        "        model = torch.compile(model)\n",
        "        return model\n",
        "    model = NextWord(block_size, len(stoi), emb_dim, hidden_size, activation).to(device)\n",
        "    state_dict = torch.load(path)\n",
        "    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "  torch.manual_seed(42)\n",
        "  emb = torch.nn.Embedding(len(stoi), emb_dim)\n",
        "  model = load_model(path = path)\n",
        "  print(\"Compiled\")\n",
        "  torch.save(model.state_dict(), f'{model_name}.pth')\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "  # Mini-batch training\n",
        "  batch_size = 4096\n",
        "  print_every = 1\n",
        "  elapsed_time = []\n",
        "  for epoch in range(epochs):\n",
        "      start_time = time.time()\n",
        "      for i in range(0, X.shape[0], batch_size):\n",
        "          x = X[i:i+batch_size]\n",
        "          y = Y[i:i+batch_size]\n",
        "          y_pred = model(x)\n",
        "          loss = loss_fn(y_pred, y)\n",
        "          loss.backward()\n",
        "          opt.step()\n",
        "          opt.zero_grad()\n",
        "          print(f\"Model: {model_name} {int(i/batch_size)} Minibatch out of {int(len(X)/batch_size)} done, {epoch} done, loss: {loss.item()}\")\n",
        "      end_time = time.time()\n",
        "      elapsed_time.append(end_time - start_time)\n",
        "      print(f\"Model: {model_name} Epoch {epoch} done, elapsed time: {end_time - start_time}\")\n",
        "      if epoch % print_every == 0:\n",
        "          print(epoch, loss.item())\n",
        "          torch.save(model.state_dict(), f'{model_name}.pth')\n",
        "  torch.save(model.state_dict(), f'{model_name}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now train all models using the dataset we have made, and save them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mE3t2_HEhAaw",
        "outputId": "cd6858a4-48d1-46fb-b35e-cc09e2af9d28"
      },
      "outputs": [],
      "source": [
        "hidden_size = [512, 512, 512]\n",
        "# EMB: 128, block_size: 16, Act: tanH, Arch: 512, 512, 512\n",
        "for activation in [nn.LeakyReLU(), nn.Tanh()]:\n",
        "  for j in [128,256]:\n",
        "    for k in [16,32]:\n",
        "      if(k == 16 and activation == nn.LeakyReLU): continue\n",
        "      makemodel(f'{activation}_EMB{j}_Context{k}', block_size = k, emb_dim = j, hidden_size = [512,512,512], activation = activation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
